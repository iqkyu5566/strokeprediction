# -*- coding: utf-8 -*-
"""Stroke Prediction Analytics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GDp_PzlTDJ-ItzFpx9XZv3BjA4Zr8hnU

# Proyek Machine Learning : Predictive Analytics
**Domain** : Kesehatan

**Tujuan** : Tujuan dari dataset ini adalah untuk memprediksi kemungkinan seseorang mengalami stroke berdasarkan informasi karakteristik dan kondisi kesehatannya, seperti jenis kelamin, usia, riwayat penyakit tertentu (seperti diabetes atau hipertensi), dan status merokok. Dengan menggunakan dataset ini, dapat dikembangkan model prediksi yang membantu para profesional kesehatan dalam mengidentifikasi individu yang berisiko tinggi terkena stroke, sehingga mereka bisa mendapatkan penanganan dan pencegahan yang lebih tepat dan lebih dini.

**Dataset yang digunakan** : https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

from sklearn.preprocessing import OneHotEncoder
from sklearn.decomposition import PCA
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import mean_squared_error
from pandas import read_csv

"""# DATA UNDERSTANDING

**Memuat Dataset Engineering_graduate_salary.csv pada variabel "df"**
"""

#!/bin/bash
!kaggle datasets download fedesoriano/stroke-prediction-dataset -p "/content/drive/MyDrive/" --unzip

df = pd.read_csv('/content/drive/MyDrive/healthcare-dataset-stroke-data.csv')

"""**Menampilkan tipe data setiap kolom pada dataset "df"**"""

df.info()

"""**Menghapus(drop) kolom/fitur yang tidak diperlukan**"""

df.drop(
    ['id'],
    axis='columns',
    inplace=True
)

"""**Menampilkan tipe data setiap kolom pada dataset "df" setelah proses drop**"""

df.info()

"""**Deskripsi Variabel Numerik**"""

df.describe()

"""**Mendapatkan ukuran(shape) dari dataset**"""

df.shape

"""**Melakukan pemeriksaan terhadap nilai yang hilang(missing value) pada dataset**"""

df.isnull().sum()

"""Menghapus Baris dengan Nilai NaN"""

df.dropna(subset=['bmi'], inplace=True)

df.isnull().sum()

"""Memvisualisasikan data menggunakan boxplot untuk fitur numerik: [age,hypertension, heart_disease, avg_glucose_level, bmi]"""

sns.boxplot(x=df['age'])

sns.boxplot(x=df['avg_glucose_level'])

sns.boxplot(x=df['bmi'])

sns.boxplot(x=df['hypertension'])

sns.boxplot(x=df['heart_disease'])

df.shape

df.info()

df.head()

"""Menganalisa data menggunakan Univariate Analysis
Membagi fitur numerik dan kategorik yang terdapat pada dataset
"""

category_feature = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']
numberic_feature = ['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi', 'stroke']

"""Melakukan analisa fitur kategori"""

feature = category_feature[0]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df1 = pd.DataFrame({
    'Count': count,
    'Percent': percent.round(1)
})
print(df1)
count.plot(kind='bar', title=feature)

df = df[df['gender'] != 'Other']

"""Menganalisa fitur gender dimana gender other sudah dihilangkan"""

feature = category_feature[0]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df1 = pd.DataFrame({
    'Count': count,
    'Percent': percent.round(1)
})
print(df1)
count.plot(kind='bar', title=feature)

"""menganalisa fitur ever maried"""

feature = category_feature[1]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df1 = pd.DataFrame({
    'Count': count,
    'Percent': percent.round(1)
})
print(df1)
count.plot(kind='bar', title=feature)

"""Menganalisa fitur work_type"""

feature = category_feature[2]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df1 = pd.DataFrame({
    'Count': count,
    'Percent': percent.round(1)
})
print(df1)
count.plot(kind='bar', title=feature)

"""Menganalisa feature residence type"""

feature = category_feature[3]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df1 = pd.DataFrame({
    'Count': count,
    'Percent': percent.round(1)
})
print(df1)
count.plot(kind='bar', title=feature)

"""Menganalisa feature smoking status"""

feature = category_feature[4]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df1 = pd.DataFrame({
    'Count': count,
    'Percent': percent.round(1)
})
print(df1)
count.plot(kind='bar', title=feature)

"""Menganalisa data menggunakan Multivariate Analysis"""

#menganalisa data feature categoric dan numeric
category = df.select_dtypes(include=['object']).columns.tolist()

for col in category:
  sns.catplot(x=col, y="stroke", kind="bar", dodge=False, height = 4,
              aspect = 5, data=df, palette="Set3")

"""Menampilkan Plot Pair fitur numerik"""

sns.pairplot(df, diag_kind='kde')

"""Melakukan pengamatan terhadap tingkat korelasi dengan menggunakan matrik korelasi pada tiap fitur"""

category_feature = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']
#konversi category_feature ke numberic
df[category_feature] = df[category_feature].apply(lambda col: pd.Categorical(col).codes)

plt.figure(figsize=(10, 8))
correlation_matrix = df.corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidth=0.5)
plt.title("Matrik Korelasi fitur numerik", size=20)

"""### **DATA PREPARATION**

Langkah selanjutnya melakukan pembagian data atau split data pada data train dengan perbandingan 80:20, kemudian melakukan standarisasi data.
"""

#pembagian data train sebesar 80:20
from sklearn.model_selection import train_test_split
X = df.drop(['stroke'], axis=1)
y = df['stroke']
from sklearn.model_selection import train_test_split

# Bagi dataset menjadi 80% data latih dan 20% data uji
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Cek jumlah data latih dan uji
print(f"Jumlah data latih: {len(X_train)}")
print(f"Jumlah data uji: {len(X_test)}")

#Melakukan standarisasi dengan StandardScaler pada data latih carat, table, dimension.
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Menampilkan Hasil standarisasi dengan tabel
X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)
X_train_scaled.head()

"""### **Model Development**

Dalam melakukan pemodelan Stroke Prediction Analytics Klasifikasi saya memilih model klasifikasi karena variabel target berupa kalsifikasi rentang nilai 0 - 1 yang menentukan apakah seseorang akan mengalami stroke atau tidak.

Adapun model klasifikasi yang akan saya pilih adalah Random Forest klasifikasi dan Support Vektor Machine dengan melakukan optimasi pada kedua model tersebut menggunakan Hyperparameter GridSearch.

1. Model Random Forest
"""

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.ensemble import RandomForestClassifier

# Inisialisasi model
model_rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Latih model dengan data latih
model_rf.fit(X_train_scaled, y_train)

"""**Evaluasi Model Random Forest**"""

# Prediksi pada data uji
y_pred_rf = model_rf.predict(X_test_scaled)

# Evaluasi model
print(classification_report(y_test, y_pred_rf))
print(confusion_matrix(y_test, y_pred_rf))

"""Dari hasil evaluasi diatas model random forest memiliki hasil akurasi sebesar 95%. hasil ini akan kita tingkatkan dengan optimasi menggunakan Hyperparameter GridSearch.

**Confusion Matrix.**
"""

#Melihat Confusion Matrix dengan warna cerah
cm = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""2. Hyperparameter Model Random Forest dengan GridSearch

Parameter yang digunakan untuk optimasi model random forest menggunakan GridSearch yaitu:

'n_estimators': [50, 100, 200]
'max_depth': [None, 10, 20, 30]
'min_samples_split': [2, 5, 10]
dari parameter diatas akan dicari nilai parameter terbaik menggunakan GridSearch untuk model klasifikasi random forest.
"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(estimator=model_rf, param_grid=param_grid, cv=5)
grid_search.fit(X_train_scaled, y_train)

# Best parameters dan evaluasi ulang model
print(grid_search.best_params_)
y_pred_best_rf = grid_search.best_estimator_.predict(X_test_scaled)
print(classification_report(y_test, y_pred_best_rf))

"""Hasil parameter terbaik dari Hyperparameter GridSearch yaitu:

'max_depth': 20
'min_samples_split': 5
'n_estimators': 200
Dari hasil optimasi menggunakan Hyperparameter GridSearch dapat diketahui peningkatan dari hasil akurasi sebesar 3% yaitu dari 89% menjadi 91%. Peningkatan ini tejadi dari parameter terbaik yang dihasilkan.
"""

#Melihat Confusion Matrix dengan warna cerah
cm = confusion_matrix(y_test, y_pred_best_rf)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

"""3. Model Support Vektor Machine"""

from sklearn.svm import SVC
# Inisialisasi model SVM
model_svm = SVC(kernel='rbf', random_state=42)

# Latih model
model_svm.fit(X_train, y_train)

# Prediksi data uji
from sklearn.metrics import accuracy_score, classification_report

y_pred_svm = model_svm.predict(X_test)

# Evaluasi performa model
print(f"Akurasi: {accuracy_score(y_test, y_pred_svm)}")
print(classification_report(y_test, y_pred_svm))

"""Dari hasil evaluasi diatas model SVM memiliki hasil akurasi sebesar 96%. hasil yang cukup tinggi dan melebihi nilai akurasi dari model random forest. Selanjutnya hasil ini akan kita tingkatkan dengan optimasi yang sama sebelumnya yaitu menggunakan Hyperparameter GridSearch.

**Confusion Matrix**
"""

#Confusion Matrix Model SVM
cm = confusion_matrix(y_test, y_pred_svm)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')